# ðŸŽ® Configurar LLM Playground no MCP Inspector

Guia para usar o playground com seu servidor MCP

---

## ðŸŽ¯ O Problema

VocÃª adicionou a API key da OpenAI no Inspector, mas o playground nÃ£o funciona ou pede para criar conta.

**Causa**: O MCP Inspector tem limitaÃ§Ãµes com algumas integraÃ§Ãµes de LLM.

---

## âœ… SoluÃ§Ãµes DisponÃ­veis

### ðŸ†“ OpÃ§Ã£o 1: Ollama (Recomendado - Gratuito e Local)

**Vantagens**:
- âœ… Totalmente gratuito
- âœ… Roda localmente (privacidade)
- âœ… NÃ£o precisa de API key
- âœ… Funciona offline

**InstalaÃ§Ã£o**:

#### Ubuntu/Debian (seu caso)

```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Verificar instalaÃ§Ã£o
ollama --version

# Baixar modelo (escolha um):
ollama pull llama3.2          # 2GB - RÃ¡pido
ollama pull llama3.2:3b       # 3GB - Balanceado
ollama pull mistral           # 4GB - Bom para cÃ³digo
ollama pull codellama         # 4GB - Especializado em cÃ³digo

# Testar
ollama run llama3.2
```

#### Iniciar Inspector com Ollama

```bash
# Parar Inspector atual se estiver rodando
lsof -ti:3000 | xargs kill -9 2>/dev/null

# Iniciar com Ollama
npx @mcpjam/inspector@latest --config mcp.json --ollama llama3.2
```

Agora o playground funcionarÃ¡ automaticamente! ðŸŽ‰

---

### ðŸ’° OpÃ§Ã£o 2: OpenAI API (Paga)

Se vocÃª tem crÃ©ditos na OpenAI e quer usar GPT-4/GPT-3.5:

#### Passo 1: Obter API Key

1. Acesse: https://platform.openai.com/api-keys
2. Clique em **"Create new secret key"**
3. Copie a chave (comeÃ§a com `sk-...`)

#### Passo 2: Adicionar ao .env.local

```bash
echo "" >> .env.local
echo "# OpenAI API Key para LLM Playground" >> .env.local
echo "OPENAI_API_KEY=sk-sua-chave-aqui" >> .env.local
```

#### Passo 3: Configurar no Inspector

O Inspector pode nÃ£o ler automaticamente do `.env.local`. VocÃª tem duas opÃ§Ãµes:

**OpÃ§Ã£o A: Via Interface**
1. Abra o Inspector
2. VÃ¡ em **Settings** (Ã­cone de engrenagem)
3. Cole sua API key no campo **OpenAI API Key**
4. Salve

**OpÃ§Ã£o B: Via VariÃ¡vel de Ambiente**
```bash
export OPENAI_API_KEY="sk-sua-chave-aqui"
npx @mcpjam/inspector@latest --config mcp.json
```

---

### ðŸ¤– OpÃ§Ã£o 3: Anthropic Claude (Paga)

Se vocÃª prefere Claude:

#### Passo 1: Obter API Key

1. Acesse: https://console.anthropic.com/settings/keys
2. Crie uma nova API key
3. Copie a chave

#### Passo 2: Adicionar ao .env.local

```bash
echo "" >> .env.local
echo "# Anthropic API Key para Claude" >> .env.local
echo "ANTHROPIC_API_KEY=sk-ant-sua-chave-aqui" >> .env.local
```

#### Passo 3: Iniciar Inspector

```bash
export ANTHROPIC_API_KEY="sk-ant-sua-chave-aqui"
npx @mcpjam/inspector@latest --config mcp.json
```

---

## ðŸ§ª Testar o Playground

### 1. Verificar ConexÃ£o

ApÃ³s iniciar o Inspector:

1. Abra: http://localhost:3000
2. Verifique se o servidor "sati" estÃ¡ **Connected** (verde)
3. Clique na aba **"LLM Playground"**

### 2. Teste Simples

Digite no chat:

```
OlÃ¡! VocÃª estÃ¡ conectado ao servidor MCP do Sati?
```

O LLM deve responder e ter acesso Ã s suas 10 tools.

### 3. Teste com Tool

```
Crie um hyperfocus chamado "Testar MCP Inspector" com cor azul e 30 minutos estimados
```

O LLM deve:
1. Chamar a tool `createHyperfocus`
2. Retornar o resultado estruturado
3. Mostrar o componente visual

---

## ðŸŽ¯ RecomendaÃ§Ã£o: Ollama

Para desenvolvimento e testes, **recomendo fortemente usar Ollama**:

### Por quÃª?

1. **Gratuito**: Sem custos de API
2. **RÃ¡pido**: Roda localmente
3. **Privado**: Seus dados nÃ£o saem da mÃ¡quina
4. **Offline**: Funciona sem internet

### Modelos Recomendados

| Modelo | Tamanho | Uso | Velocidade |
|--------|---------|-----|------------|
| `llama3.2` | 2GB | Geral | âš¡âš¡âš¡ |
| `llama3.2:3b` | 3GB | Balanceado | âš¡âš¡ |
| `mistral` | 4GB | CÃ³digo | âš¡âš¡ |
| `codellama` | 4GB | CÃ³digo especializado | âš¡ |

### InstalaÃ§Ã£o RÃ¡pida

```bash
# Instalar
curl -fsSL https://ollama.com/install.sh | sh

# Baixar modelo
ollama pull llama3.2

# Testar
ollama run llama3.2
# Digite: "OlÃ¡, vocÃª funciona?"
# Ctrl+D para sair

# Usar com Inspector
npx @mcpjam/inspector@latest --config mcp.json --ollama llama3.2
```

---

## ðŸ› Troubleshooting

### Erro: "Ollama not found"

**SoluÃ§Ã£o**:
```bash
# Verificar se Ollama estÃ¡ no PATH
which ollama

# Se nÃ£o estiver, adicionar ao PATH
echo 'export PATH=$PATH:/usr/local/bin' >> ~/.bashrc
source ~/.bashrc
```

### Erro: "Model not found"

**SoluÃ§Ã£o**:
```bash
# Listar modelos instalados
ollama list

# Se vazio, baixar modelo
ollama pull llama3.2
```

### Erro: "OpenAI API key invalid"

**Causas possÃ­veis**:
1. Chave copiada incorretamente (espaÃ§os extras)
2. Chave revogada
3. Sem crÃ©ditos na conta OpenAI

**SoluÃ§Ã£o**:
```bash
# Testar chave manualmente
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"

# Deve retornar lista de modelos
```

### Playground nÃ£o aparece

**SoluÃ§Ã£o**:
1. Limpar cache do navegador (Ctrl+Shift+Delete)
2. Reiniciar Inspector
3. Tentar em aba anÃ´nima

---

## ðŸ“Š ComparaÃ§Ã£o de OpÃ§Ãµes

| OpÃ§Ã£o | Custo | Velocidade | Qualidade | Privacidade | Offline |
|-------|-------|------------|-----------|-------------|---------|
| **Ollama** | ðŸ†“ GrÃ¡tis | âš¡ RÃ¡pido | â­â­â­ | âœ… Alta | âœ… Sim |
| **OpenAI** | ðŸ’° Pago | âš¡âš¡âš¡ Muito rÃ¡pido | â­â­â­â­â­ | âŒ Baixa | âŒ NÃ£o |
| **Claude** | ðŸ’° Pago | âš¡âš¡ RÃ¡pido | â­â­â­â­â­ | âŒ Baixa | âŒ NÃ£o |

---

## ðŸš€ Script de Setup Completo

```bash
#!/bin/bash

# Setup completo do LLM Playground

echo "ðŸŽ® Configurando LLM Playground..."

# OpÃ§Ã£o 1: Ollama (recomendado)
read -p "Instalar Ollama? (s/n): " install_ollama
if [ "$install_ollama" = "s" ]; then
    echo "ðŸ“¥ Instalando Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
    
    echo "ðŸ“¦ Baixando modelo llama3.2..."
    ollama pull llama3.2
    
    echo "âœ… Ollama instalado!"
    echo ""
    echo "Para usar:"
    echo "  npx @mcpjam/inspector@latest --config mcp.json --ollama llama3.2"
fi

# OpÃ§Ã£o 2: OpenAI
read -p "Configurar OpenAI API? (s/n): " setup_openai
if [ "$setup_openai" = "s" ]; then
    read -p "Cole sua OpenAI API key: " openai_key
    echo "" >> .env.local
    echo "OPENAI_API_KEY=$openai_key" >> .env.local
    echo "âœ… OpenAI configurada!"
fi

echo ""
echo "ðŸŽ‰ Setup completo!"
```

Salve como `setup-playground.sh` e execute:
```bash
chmod +x setup-playground.sh
./setup-playground.sh
```

---

## ðŸ“š Recursos

- [Ollama](https://ollama.com) - Site oficial
- [Ollama Models](https://ollama.com/library) - CatÃ¡logo de modelos
- [OpenAI API](https://platform.openai.com) - Dashboard
- [Anthropic Console](https://console.anthropic.com) - Claude API

---

## ðŸŽ¯ Resumo RÃ¡pido

```bash
# SoluÃ§Ã£o mais rÃ¡pida: Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2
npx @mcpjam/inspector@latest --config mcp.json --ollama llama3.2

# Abrir: http://localhost:3000
# Ir para: LLM Playground
# Testar: "Crie um hyperfocus chamado Teste"
```

---

**Ãšltima atualizaÃ§Ã£o**: 2025-10-09  
**RecomendaÃ§Ã£o**: Use Ollama para desenvolvimento! ðŸš€

